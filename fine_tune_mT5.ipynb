{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EO6N5rPKfCgw"
      },
      "outputs": [],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uy59OTtb6Fjn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "# import os\n",
        "# os.chdir('/content/drive/My Drive/translate')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uF1yKN_XrQi"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYEjq_MCeenr"
      },
      "outputs": [],
      "source": [
        "!sudo apt install rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EC3W615OjEE1"
      },
      "outputs": [],
      "source": [
        "!rar x datasets.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqIXBUCS6OL1"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets evaluate sacrebleu nltk keras_nlp\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6BZk-9Z4aQ0"
      },
      "outputs": [],
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import TFAutoModelForSeq2SeqLM\n",
        "from transformers import AdamWeightDecay\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import tensorflow as tf\n",
        "# import keras_nlp\n",
        "# from nltk.translate.bleu_score import sentence_bleu\n",
        "# import evaluate\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZITxfx-4kiv"
      },
      "outputs": [],
      "source": [
        "split_char = '⫯'\n",
        "data_name = 'PTT_2023_08_06'\n",
        "model_name = f'mT5_small_PTT_2023_08_06_Thu_Aug_17_081645_2023.ckpt'\n",
        "# model_name = f'mT5_small_{data_name}_{time.ctime().replace(\" \", \"_\").replace(\":\", \"\")}.ckpt'\n",
        "checkpoint = \"google/mt5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "991cHkgKtT4V"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkvUVM4Zn-9U"
      },
      "outputs": [],
      "source": [
        "prefix = \"translate engTyping to Traditional Chinese:\".split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwlEOfPyA7pD"
      },
      "outputs": [],
      "source": [
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWF5gHsKYk_Q"
      },
      "outputs": [],
      "source": [
        "model.load_weights('models/mT5_small_PTT_2023_08_06_Thu_Aug_17_081645_2023.ckpt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poQuv6JatPMN"
      },
      "outputs": [],
      "source": [
        "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbC9afFE5xzt"
      },
      "outputs": [],
      "source": [
        "tf_train_set = tf.data.Dataset.load(f'datasets/mT5_{data_name}_tf_train_dataset.tfrecord')\n",
        "tf_val_set = tf.data.Dataset.load(f'datasets/mT5_{data_name}_tf_val_dataset.tfrecord')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykQKo6Qwc96f"
      },
      "outputs": [],
      "source": [
        "tf.config.run_functions_eagerly(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9AuwNXL50kT"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=optimizer, metrics=[\"accuracy\"])  # No loss argument!\n",
        "\n",
        "# metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_val_set)\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(f'models/{model_name}',save_best_only=True, save_weights_only=True),\n",
        "    tf.keras.callbacks.TensorBoard(log_dir=f'logs/{model_name}_logs')\n",
        "]\n",
        "\n",
        "model.fit(x=tf_train_set, validation_data=tf_val_set, epochs=1, callbacks=callbacks)\n",
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir('/content/drive/My Drive/translate')\n",
        "model.save_pretrained(f'models/{model_name}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBRwgyZzrano"
      },
      "outputs": [],
      "source": [
        "split_char = '⫯'\n",
        "punctuations = '、，。？！：；'\n",
        "\n",
        "def engTyping_insert_split_char(sentence: str, split_char: str) -> str:\n",
        "    insert_times = 0\n",
        "    sentence_list = list(sentence)\n",
        "    for i, char in enumerate(sentence):\n",
        "        if char in ' 6347' + punctuations:\n",
        "            sentence_list.insert(i + insert_times + 1, split_char)\n",
        "            insert_times += 1\n",
        "    return ''.join(sentence_list[:-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uy948uP755ey"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(prefix + engTyping_insert_split_char(input('?:'), split_char).split(split_char), is_split_into_words=True, return_tensors=\"tf\", truncation=True).input_ids\n",
        "outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
