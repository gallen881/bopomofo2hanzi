# from colab

# -*- coding: utf-8 -*-
"""fine-tune_mT5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MDSJtMPldX9B25Fb1b8E8MIr2ogTDQR7
"""

# gpu_info = !nvidia-smi
# gpu_info = '\n'.join(gpu_info)
# if gpu_info.find('failed') >= 0:
#   print('Not connected to a GPU')
# else:
#   print(gpu_info)

# from google.colab import drive
# import os
# drive.mount('/content/drive')
# os.chdir('/content/drive/My Drive/translate')

# !pip install transformers datasets evaluate sacrebleu
# !pip install sentencepiece

for _ in range(3):
    print('From Colab')

from transformers import TFAutoModelForSeq2SeqLM
from transformers import AutoTokenizer
import tensorflow as tf
import os

os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'

split_char = '⫯'
punctuations = '、，。？！：；'
data_name = 'PTT_2023_08_06'
model_name = f''
checkpoint = "google/mt5-small"

model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
model.load_weights(r'C:\Users\auer\Downloads\mT5_small_PTT_2023_08_06_Thu_Aug_17_081645_2023.ckpt')
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

prefix = "translate engTyping to Traditional Chinese:".split()


def engTyping_insert_split_char(sentence: str, split_char: str) -> str:
    insert_times = 0
    sentence_list = list(sentence)
    for i, char in enumerate(sentence):
        if char in ' 6347' + punctuations:
            sentence_list.insert(i + insert_times + 1, split_char)
            insert_times += 1
    return ''.join(sentence_list[:-1])

while True:
    inputs = tokenizer(prefix + engTyping_insert_split_char(input('?:'), split_char).split(split_char), is_split_into_words=True, return_tensors="tf", truncation=True).input_ids
    outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))